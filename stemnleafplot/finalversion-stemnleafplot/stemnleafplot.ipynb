{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")  # Removes all punctuation marks in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import all of the statements from the tokenText folder and extract the root and derivatives of root words\n",
    "'''\n",
    "\n",
    "tokenized_statements = []\n",
    "textdir = (r\"C:\\Users\\manalais\\decode-diversity-statements\\texts\")\n",
    "for filename in os.listdir(textdir)[:]:\n",
    "    filepath = os.path.join(textdir, filename)\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        tokens = tokenizer.tokenize(text)  # Returns a text as a list of words with punctuations\n",
    "        for word in tokens:\n",
    "            tokenized_statements.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the uniquewordslist.txt which will be used for the root words\n",
    "'''\n",
    "\n",
    "textdir = (r'C:\\Users\\manalais\\decode-diversity-statements')\n",
    "filename = \"uniquewordslist.txt\"\n",
    "\n",
    "text = []\n",
    "with open(os.path.join(textdir, filename), 'r') as file:\n",
    "        text = file.read()\n",
    "        text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokenized_statements) # tag all the statements with the appropriate POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "roots_and_leaves = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' go through each word in tagged_tokens. Lemmatize earch word and add it to the dictionary with its leaves'''\n",
    "\n",
    "for word, tag in tagged_tokens:\n",
    "    if  not tag.startswith('NNP' or 'NNPS'): # remove propr nouns\n",
    "        if tag.startswith('NN' or 'NNS'):\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB' or 'VBP' or 'VBD' or 'VBG' or 'VBP' or 'VBZ'):\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ' or 'JJR' or 'JJS' ):\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('RB' or 'RBR' or 'RBS' or 'RP'):\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos='r')\n",
    "        else:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)   \n",
    "    \n",
    "        if lemmatized_word in roots_and_leaves.keys():\n",
    "            if word not in roots_and_leaves[lemmatized_word]:\n",
    "                roots_and_leaves[lemmatized_word].append(word)\n",
    "        else:\n",
    "            roots_and_leaves[lemmatized_word] = []\n",
    "            roots_and_leaves[lemmatized_word].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''alphabetically sort dictionary'''\n",
    "roots_and_leaves = dict(sorted(roots_and_leaves.items(), key=lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create a list of root  words that is not in the uniquewordlist.txt file'''\n",
    "unimportant_roots = []\n",
    "for key in roots_and_leaves.keys():\n",
    "    if key not in text:\n",
    "        unimportant_roots.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' remove all the roots and its leaves that is not in uniquewordlist.txt file '''\n",
    "for root in unimportant_roots:\n",
    "    roots_and_leaves.pop(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Print the output'''\n",
    "print(\"\\n\" + \"\\033[1m\" + \"Root and Leaf Plot\".center(60) + \"\\n\")\n",
    "for root,leaves in roots_and_leaves.items():\n",
    "    print(\"\\033[1m\" + root + \"\\033[0m\" + '| ' + \" \".join(leaves)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(Option 1) Write dictionary to a text file'''\n",
    "with open(\"stemnleaf.json\", \"w\") as file:  \n",
    "    json.dump(roots_and_leaves, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(Option 2) Visualize the data and save it as Markdown document'''\n",
    "\n",
    "sys.stdout = open(r'C:\\Users\\manalais\\decode-diversity-statements\\stemnleafplot\\stemnleafplot.md', 'w')\n",
    "print(\"**\" + \"Root and Leaf Plot\" + \"**\" + \"\\n\")\n",
    "for root,leaves in roots_and_leaves.items():\n",
    "    print(\"**\" + root + \"**\" + \"|\" + \"  \" + \" \".join(leaves) + \"   \")      \n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(Option 3) Visualize the data and save it as txt.file'''\n",
    "\n",
    "sys.stdout = open(r'C:\\Users\\manalais\\decode-diversity-statements\\stemnleafplot\\stemnleafplot.txt', 'w')\n",
    "print(\"Root and Leaf Plot\" + \"\\n\")\n",
    "for root,leaves in roots_and_leaves.items():\n",
    "    print(root + '| ' + \" \".join(leaves))      \n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
