{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "import difflib\n",
    "import pandas as pd\n",
    "nltk.download('words')\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "def removePunctuation(sentence):\n",
    "    '''Input must be string. Tokenizes and removes punctuation.'''   \n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    punctuations=\"\\\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~â€“\"\n",
    "    for word in tokenized_sentence:\n",
    "        if word in punctuations:\n",
    "            tokenized_sentence.remove(word)\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove StopWords\n",
    "def removeStopWords(sentence):\n",
    "    '''takes tokenized words as an input and removes all the stop words''' \n",
    "    for word in sentence:\n",
    "        if word in stopwords.words('english'):\n",
    "            sentence.remove(word)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2d02b1ca6473>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlemma_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "'''The code below returns a list of 20435 lemma words in English'''\n",
    "\n",
    "file = open(r'C:\\Users\\manalais\\Box\\Web Developer\\root_words.txt')\n",
    "line = file.readlines()\n",
    "for word in line:\n",
    "    word = nltk.word_tokenize(word)\n",
    "    words.append(word)\n",
    "lemma_words = []\n",
    "for sublist in words:\n",
    "    for item in sublist:\n",
    "        lemma_words.append(item)\n",
    "print(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''The code below is used for stemming and un-stemming'''\n",
    "\n",
    "stem_list = []\n",
    "root_words = []\n",
    "root_list = []\n",
    "\n",
    "# This code removes punctuation, stopwords and rturns it as a list\n",
    "my_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\decode-diversity-statements\\texts\\AAAS_(American_Association_for_the_Advancement_of_Science).txt\", encoding=\"utf-8\")\n",
    "for line in my_file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    rem_ove = removePunctuation(line)\n",
    "    noStop = removeStopWords(rem_ove)\n",
    "    for words in noStop:\n",
    "        root_list.append(words)\n",
    "print(root_list)  \n",
    "\n",
    "#stem the words extracted from the article\n",
    "for word in root_list:\n",
    "    stem_list.append(porter.stem(word))\n",
    "\n",
    "print('After stemming: ')\n",
    "print(stem_list)\n",
    "\n",
    "# iterates through the lemma words list to unstem the words to their original form\n",
    "for c in stem_list:\n",
    "    for w in lemma_words:\n",
    "        similiarity = difflib.SequenceMatcher(None, c, w).ratio()\n",
    "        if similiarity > 0.91:\n",
    "            root_words.append(w)\n",
    "print('After unstemming: ')\n",
    "print(root_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dic = dict()\n",
    "leaf_list = []\n",
    "\n",
    "#create a dictionory here assign all the roots to keys and empty list as values\n",
    "for i in root_words:\n",
    "    dic[i] = []\n",
    "   \n",
    "# This code finds the leaf words and returns it as a list  \n",
    "token_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\decode-diversity-statements\\texts\\AAAS_(American_Association_for_the_Advancement_of_Science).txt\", encoding=\"utf-8\")\n",
    "for line in token_file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    rem_ove = removePunctuation(line)\n",
    "    noStop = removeStopWords(rem_ove)\n",
    "    for words in noStop:\n",
    "        leaf_list.append(words)\n",
    "        \n",
    "leaf_list = list(set(leaf_list))  # removes the duplicate words\n",
    "\n",
    "\n",
    " \n",
    "# This code matches leaf words to root words \n",
    "for root in root_words:\n",
    "    for leaf in leaf_list:\n",
    "        similiarity = difflib.SequenceMatcher(None, root, leaf).ratio()\n",
    "        if similiarity > 0.75:\n",
    "            dic[root].append(leaf)\n",
    "\n",
    "\n",
    "#Task: visualization: print out the roots and leaves\n",
    "key_list = []\n",
    "value_list = []\n",
    "\n",
    "for k in dic.keys():\n",
    "    key_list.append(k)\n",
    "for v in dic.values():\n",
    "    value_list.append(v)\n",
    "print(\"\\033[1m\" + \"Root and Leaf Plot\".center(60) + \"\\n\")\n",
    "for key,value in zip(key_list,value_list):\n",
    "    print (\"\\033[1m\" + key + \"\\033[0m\" + ' | ' + \" \".join(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
