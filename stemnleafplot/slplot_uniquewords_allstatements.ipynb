{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Date: Thursday, January 14, 2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manalais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\manalais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "import difflib\n",
    "import pandas as pd\n",
    "nltk.download('words')\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "import os\n",
    "from nltk.tag import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory\n",
      "vandalism\n",
      "violently\n",
      "condolences\n",
      "slavery\n",
      "antiracist\n",
      "kneeling\n",
      "blacklivesmatter\n",
      "anti-racism\n",
      "inequalities\n",
      "identities\n",
      "shooting\n",
      "protesting\n",
      "inequity\n",
      "died\n",
      "violent\n",
      "struggle\n",
      "activities\n",
      "anti-racist\n",
      "minority\n",
      "loss\n",
      "force\n",
      "humanity\n",
      "killed\n",
      "grief\n",
      "inequities\n",
      "white \n",
      "voices\n",
      "pain\n",
      "systemic\n",
      "minneapolis\n",
      "discrimination\n",
      "american\n",
      "respect\n",
      "arbery\n",
      "action\n",
      "social\n",
      "lives\n",
      "racial\n",
      "color\n",
      "americans\n",
      "ahmaud\n",
      "black\n",
      "anger\n",
      "values\n",
      "inclusive\n",
      "commitment\n",
      "pandemic\n",
      "past\n",
      "taylor\n",
      "life\n",
      "breonna\n",
      "issues\n",
      "rights\n",
      "race\n",
      "right\n",
      "fear\n",
      "protests\n",
      "challenges\n",
      "deaths\n",
      "hate\n",
      "officers\n",
      "sadness\n",
      "killing\n",
      "injustices\n",
      "video\n",
      "equality\n",
      "civil\n",
      "oppression\n",
      "crisis\n",
      "killings\n",
      "policies\n",
      "protest\n",
      "victims\n",
      "inequality\n",
      "murder\n",
      "ethnicity\n",
      "skin\n",
      "government\n",
      "underrepresented\n",
      "unrest\n",
      "hurt\n",
      "frustration\n",
      "minnesota\n",
      "angry\n",
      "movement\n",
      "african-american\n",
      "privilege\n",
      "tragedies\n",
      "mourn\n",
      "murdered\n",
      "arrested\n",
      "demonstrations\n",
      "problem\n",
      "historically\n",
      "unarmed\n",
      "vulnerable\n",
      "mourning\n",
      "threat\n",
      "recently\n",
      "suffered\n",
      "advocacy\n",
      "anti-asian\n",
      "unprecedented\n",
      "success\n",
      "cases\n",
      "aware\n",
      "urgent\n",
      "represent\n",
      "pursuit\n",
      "criminal\n",
      "respectful\n",
      "unjust\n",
      "posts\n",
      "George\n",
      "Floyd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Import the uniquewordslist.txt which will be used for the root words\n",
    "'''\n",
    "\n",
    "           \n",
    "textdir = (r'C:\\Users\\manalais\\decode-diversity-statements')\n",
    "filename = \"uniquewordslist.txt\"\n",
    "\n",
    "text = []\n",
    "with open(os.path.join(textdir, filename), 'r') as file:\n",
    "        text = file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memory', 'vandalism', 'violently', 'condolences', 'slavery', 'antiracist', 'kneeling', 'blacklivesmatter', 'anti', 'racism', 'inequalities', 'identities', 'shooting', 'protesting', 'inequity', 'died', 'violent', 'struggle', 'activities', 'anti', 'racist', 'minority', 'loss', 'force', 'humanity', 'killed', 'grief', 'inequities', 'white', 'voices', 'pain', 'systemic', 'minneapolis', 'discrimination', 'american', 'respect', 'arbery', 'action', 'social', 'lives', 'racial', 'color', 'americans', 'ahmaud', 'black', 'anger', 'values', 'inclusive', 'commitment', 'pandemic', 'past', 'taylor', 'life', 'breonna', 'issues', 'rights', 'race', 'right', 'fear', 'protests', 'challenges', 'deaths', 'hate', 'officers', 'sadness', 'killing', 'injustices', 'video', 'equality', 'civil', 'oppression', 'crisis', 'killings', 'policies', 'protest', 'victims', 'inequality', 'murder', 'ethnicity', 'skin', 'government', 'underrepresented', 'unrest', 'hurt', 'frustration', 'minnesota', 'angry', 'movement', 'african', 'american', 'privilege', 'tragedies', 'mourn', 'murdered', 'arrested', 'demonstrations', 'problem', 'historically', 'unarmed', 'vulnerable', 'mourning', 'threat', 'recently', 'suffered', 'advocacy', 'anti', 'asian', 'unprecedented', 'success', 'cases', 'aware', 'urgent', 'represent', 'pursuit', 'criminal', 'respectful', 'unjust', 'posts', 'George', 'Floyd']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TOKENIZE TEXT\n",
    "'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\") #regex to get all words, including numbers, but not punctuaction\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memory', 'vandalism', 'violently', 'condolences', 'slavery', 'antiracist', 'kneeling', 'blacklivesmatter', 'anti', 'racism', 'inequalities', 'identities', 'shooting', 'protesting', 'inequity', 'died', 'violent', 'struggle', 'activities', 'anti', 'racist', 'minority', 'loss', 'force', 'humanity', 'killed', 'grief', 'inequities', 'white', 'voices', 'pain', 'systemic', 'minneapolis', 'discrimination', 'american', 'respect', 'arbery', 'action', 'social', 'lives', 'racial', 'color', 'americans', 'ahmaud', 'black', 'anger', 'values', 'inclusive', 'commitment', 'pandemic', 'past', 'taylor', 'life', 'breonna', 'issues', 'rights', 'race', 'right', 'fear', 'protests', 'challenges', 'deaths', 'hate', 'officers', 'sadness', 'killing', 'injustices', 'video', 'equality', 'civil', 'oppression', 'crisis', 'killings', 'policies', 'protest', 'victims', 'inequality', 'murder', 'ethnicity', 'skin', 'government', 'underrepresented', 'unrest', 'hurt', 'frustration', 'minnesota', 'angry', 'movement', 'african', 'american', 'privilege', 'tragedies', 'mourn', 'murdered', 'arrested', 'demonstrations', 'problem', 'historically', 'unarmed', 'vulnerable', 'mourning', 'threat', 'recently', 'suffered', 'advocacy', 'anti', 'asian', 'unprecedented', 'success', 'cases', 'aware', 'urgent', 'represent', 'pursuit', 'criminal', 'respectful', 'unjust', 'posts', 'George', 'Floyd']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Remove punctuation\n",
    "'''\n",
    "\n",
    "punctuations=\"\\\"!#â$%&'()*+,-./:;<=>?@[\\]^_`{|}~–\"\n",
    "for word in tokens:\n",
    "    if word in punctuations:\n",
    "        tokens.remove(word)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tag part of speech first. result is list of (word, tag)'''\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stemming and Lemmatization\n",
    "'''\n",
    "\n",
    "    \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_words = []\n",
    "\n",
    "for words, tag in tagged_tokens:\n",
    "    if  not tag.startswith('NNP' or 'NNPS'): # remove propr nouns\n",
    "        if tag.startswith('NN' or 'NNS'):\n",
    "            clean_words.append(lemmatizer.lemmatize(words, pos='n'))\n",
    "        elif tag.startswith('NNP' or 'NNPS'):\n",
    "            clean_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB' or 'VBP' or 'VBD' or 'VBG' or 'VBP' or 'VBZ'):\n",
    "            clean_words.append(lemmatizer.lemmatize(words, pos='v'))\n",
    "        elif tag.startswith('JJ' or 'JJR' or 'JJS' ):\n",
    "            clean_words.append(lemmatizer.lemmatize(words, pos='a'))\n",
    "        elif tag.startswith('RB' or 'RBR' or 'RBS' or 'RP'):\n",
    "            clean_words.append(lemmatizer.lemmatize(words, pos='r'))\n",
    "        else:\n",
    "            clean_words.append(lemmatizer.lemmatize(words))\n",
    "\n",
    "print(clean_words)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Remove all the stop words'''\n",
    "for word in clean_words:\n",
    "    if word in stopwords.words('english'):\n",
    "        clean_words.remove(word)\n",
    "    if word == 'ahmaud' or word =='taylor' or word =='breonna' or word =='anti' or word =='arbery': # This part is manual work because the POS did not tag these names as NNP and so did not remove it\n",
    "        clean_words.remove(word)     \n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(Manual work): The list of words is not complete sentence therefore,\n",
    "lemmatization function POS does not tag the following correctly.\n",
    "'''\n",
    "# for words in clean_words:\n",
    "#     if words == \"racism\":\n",
    "#         clean_words[clean_words.index(\"racism\")] = \"race\"   \n",
    "#     if words == 'violently':\n",
    "#         clean_words[clean_words.index('violently')] = 'violent'\n",
    "#     if words == 'identities':\n",
    "#         clean_words[clean_words.index('identities')] = 'identity'\n",
    "#     if words == 'systemic':\n",
    "#         clean_words[clean_words.index('systemic')] = 'system'\n",
    "#     if words == 'racial':\n",
    "#         clean_words[clean_words.index('racial')] = 'race'\n",
    "#     if words == 'killing':\n",
    "#         clean_words[clean_words.index('killing')] = 'kill'\n",
    "#     if words =='underrepresented':\n",
    "#         clean_words[clean_words.index('underrepresented')]  = 'underrepresent'\n",
    "#     if words =='americans':\n",
    "#         clean_words[clean_words.index('americans')]  = 'american'\n",
    "#     if words =='arrested':\n",
    "#         clean_words[clean_words.index('arrested')]  = 'arrest' \n",
    "#     if words =='mourning':\n",
    "#         clean_words[clean_words.index('mourning')]  = 'mourn'\n",
    "#     if words =='racist':\n",
    "#         clean_words[clean_words.index('racist')]  = 'race'\n",
    "# clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove duplicate words'''\n",
    "clean_words = list(set(clean_words))  # removes the duplicate words\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create an empty dictionary with all the root words as its keys'''\n",
    "dic = {}\n",
    "for root in clean_words:\n",
    "    dic[root] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import all of the statements from the tokenText folder and extract the derivatives of root words\n",
    "'''\n",
    "\n",
    "#The Function stop_words was written by Kumbula\n",
    "filtered_words = []\n",
    "def stop_words(tokens):\n",
    "    \"\"\"\n",
    "    This function is for removing the stopwords from the text file.\n",
    "    Filename: Name of the file\n",
    "    return: filtered_words\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))  # Creating a List of stopwords\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)  # Add all words after stopwords have been removed\n",
    "    # print (\"Filtered_words:\",filtered_words )\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "# Go through each statement and find words that are derivatives of root words\n",
    "current_dir = os.getcwd()\n",
    "textdir = (r\"C:\\Users\\manalais\\decode-diversity-statements\\tokenText\")\n",
    "for filename in os.listdir(textdir)[:]:\n",
    "    filepath = os.path.join(textdir, filename)\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        tokens = tokenizer.tokenize(text)  # Returns a text as a list of words with punctuations\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()]  # Changes all the words into lower case        \n",
    "        for root in clean_words:\n",
    "            for leaf in tokens:\n",
    "                similiarity = difflib.SequenceMatcher(None, root, leaf).ratio()\n",
    "                if similiarity > 0.90:\n",
    "                    if leaf not in dic[root]:   #only append to dictionary if the value doesn't already exist\n",
    "                        dic[root].append(leaf)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(Manual work): The difflib.SequenceMatcher matches some words that are similiar but not leaves of the root word.\n",
    "For instance, pain --> paint. Therefore, I am removing them from the dictionary values manually '''\n",
    "\n",
    "# for value in dic.values():\n",
    "#     for i in value:\n",
    "#         if i == 'paint':\n",
    "#             value.remove(i)\n",
    "#         if i == 'posit':\n",
    "#             value.remove(i)\n",
    "#         if i == 'present':\n",
    "#             value.remove(i)\n",
    "#         if i == 'los':\n",
    "#             value.remove(i)\n",
    "#         if i == 'pat':\n",
    "#             value.remove(i)\n",
    "#         if i == 'sin':\n",
    "#             value.remove(i)\n",
    "#         if i == 'lack':\n",
    "#             value.remove(i)\n",
    "#         if i == 'back':\n",
    "#             value.remove(i)\n",
    "#         if i == 'far':\n",
    "#             value.remove(i)\n",
    "#         if i == 'ears':\n",
    "#             value.remove(i)\n",
    "#         if i == 'inclusivevt':\n",
    "#             value.remove(i)\n",
    "#         if i == 'fraction':\n",
    "#             value.remove(i)\n",
    "#         if i == 'reaction':\n",
    "#             value.remove(i)\n",
    "#         if i == 'emory':\n",
    "#             value.remove(i)\n",
    "#         if i == 'fore':\n",
    "#             value.remove(i) \n",
    "#         if i == 'bright':\n",
    "#             value.remove(i)\n",
    "#         if i == 'wright':\n",
    "#             value.remove(i)\n",
    "#         if i == 'identify':\n",
    "#             value.remove(i)\n",
    "#         if i == 'grace':\n",
    "#             value.remove(i)\n",
    "#         if i == 'chase':\n",
    "#             value.remove(i)\n",
    "#         if i == 'ceases':\n",
    "#             value.remove(i)\n",
    "#         if i == 'causes':\n",
    "#             value.remove(i) \n",
    "#         if i == 'vice':\n",
    "#             value.remove(i)\n",
    "#         if i == 'danger':\n",
    "#             value.remove(i)\n",
    "#         if i == 'treat':\n",
    "#             value.remove(i)\n",
    "#         if i == 'quality':\n",
    "#             value.remove(i)\n",
    "#         if i == 'ear':\n",
    "#             value.remove(i)\n",
    "#         if i == 'cease':\n",
    "#             value.remove(i)\n",
    "#         if i == 'cause':\n",
    "#             value.remove(i)\n",
    "#         if i == 'movement':\n",
    "#             value.remove(i)\n",
    "#         if i == 'protest':\n",
    "#             value.remove(i)\n",
    "#         if i == 'justice':\n",
    "#             value.remove(i)\n",
    "#         if i == 'nondiscrimination':\n",
    "#             value.remove(i)\n",
    "#         if i == 'america':\n",
    "#             value.remove(i)\n",
    "#         if i == 'inaction':\n",
    "#             value.remove(i)\n",
    "#         if i == 'knee':\n",
    "#             value.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' (Manual work): The dictionary contains key:value errors that needs to be fixed'''\n",
    "# dic['equality'] = [ 'equality']\n",
    "# dic['equality']\n",
    "# dic['inequity'] = ['inequity']\n",
    "# dic['inequity']\n",
    "# dic['officer'] = ['officer', 'officers']\n",
    "# dic['officer']\n",
    "# dic['race'] = ['race', 'races', 'racism', 'racist']\n",
    "# dic['race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Visualize the data'''\n",
    "\n",
    "key_list = []\n",
    "value_list = []\n",
    "\n",
    "\n",
    "for k in dic.keys():\n",
    "    key_list.append(k)\n",
    "for v in dic.values():\n",
    "    value_list.append(v)\n",
    "\n",
    "print(\"\\n\" + \"\\033[1m\" + \"Root and Leaf Plot\".center(60) + \"\\n\")\n",
    "for key,value in zip(key_list,value_list):\n",
    "    print(\"\\033[1m\" + key + \"\\033[0m\" + '| ' + \" \".join(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
