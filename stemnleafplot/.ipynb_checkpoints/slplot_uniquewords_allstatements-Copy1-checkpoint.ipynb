{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "import difflib\n",
    "import pandas as pd\n",
    "nltk.download('words')\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "import os\n",
    "from nltk.tag import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the uniquewordslist.txt which will be used for the root words\n",
    "'''\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "textdir = os.path.join(current_dir)\n",
    "filename = \"uniquewordslist.txt\"\n",
    "\n",
    "text = []\n",
    "with open(os.path.join(textdir, filename), 'r') as file:\n",
    "        text = file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TOKENIZE TEXT\n",
    "'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\") #regex to get all words, including numbers, but not punctuaction\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove punctuation\n",
    "'''\n",
    "\n",
    "punctuations=\"\\\"!#â$%&'()*+,-./:;<=>?@[\\]^_`{|}~–\"\n",
    "for word in tokens:\n",
    "    if word in punctuations:\n",
    "        tokens.remove(word)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tag part of speech first. result is list of (word, tag)'''\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stemming and Lemmatization\n",
    "'''\n",
    "\n",
    "    \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_words = []\n",
    "\n",
    "for words, tag in tagged_tokens:\n",
    "    if  not tag.startswith('NNP' or 'NNPS'): # remove propr nouns\n",
    "        if tag.startswith('NN' or 'NNS'):\n",
    "            clean_words.append(lemmatizer.lemmatize(words, pos='n'))\n",
    "        elif tag.startswith('NNP' or 'NNPS'):\n",
    "            clean_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB' or 'VBP' or 'VBD' or 'VBG' or 'VBP' or 'VBZ'):\n",
    "            clean_words.append(lemmatizer.lemmatize(words, pos='v'))\n",
    "        elif tag.startswith('JJ' or 'JJR' or 'JJS' ):\n",
    "            clean_words.append(lemmatizer.lemmatize(words, pos='a'))\n",
    "        elif tag.startswith('RB' or 'RBR' or 'RBS' or 'RP'):\n",
    "            clean_words.append(lemmatizer.lemmatize(words, pos='r'))\n",
    "        else:\n",
    "            clean_words.append(lemmatizer.lemmatize(words))\n",
    "\n",
    "print(clean_words)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Remove all the stop words'''\n",
    "for word in clean_words:\n",
    "    if word in stopwords.words('english'):\n",
    "        clean_words.remove(word)\n",
    "    if word == 'ahmaud' or word =='taylor' or word =='breonna' or word =='anti' or word =='arbery': # This part is manual work because the POS did not tag these names as NNP and so did not remove it\n",
    "        clean_words.remove(word)     \n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(Manual work): The list of words is not complete sentence therefore,\n",
    "lemmatization function POS does not tag the following correctly.\n",
    "'''\n",
    "# for words in clean_words:\n",
    "#     if words == \"racism\":\n",
    "#         clean_words[clean_words.index(\"racism\")] = \"race\"   \n",
    "#     if words == 'violently':\n",
    "#         clean_words[clean_words.index('violently')] = 'violent'\n",
    "#     if words == 'identities':\n",
    "#         clean_words[clean_words.index('identities')] = 'identity'\n",
    "#     if words == 'systemic':\n",
    "#         clean_words[clean_words.index('systemic')] = 'system'\n",
    "#     if words == 'racial':\n",
    "#         clean_words[clean_words.index('racial')] = 'race'\n",
    "#     if words == 'killing':\n",
    "#         clean_words[clean_words.index('killing')] = 'kill'\n",
    "#     if words =='underrepresented':\n",
    "#         clean_words[clean_words.index('underrepresented')]  = 'underrepresent'\n",
    "#     if words =='americans':\n",
    "#         clean_words[clean_words.index('americans')]  = 'american'\n",
    "#     if words =='arrested':\n",
    "#         clean_words[clean_words.index('arrested')]  = 'arrest' \n",
    "#     if words =='mourning':\n",
    "#         clean_words[clean_words.index('mourning')]  = 'mourn'\n",
    "#     if words =='racist':\n",
    "#         clean_words[clean_words.index('racist')]  = 'race'\n",
    "# clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove duplicate words'''\n",
    "clean_words = list(set(clean_words))  # removes the duplicate words\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create an empty dictionary with all the root words as its keys'''\n",
    "dic = {}\n",
    "for root in clean_words:\n",
    "    dic[root] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import all of the statements from the tokenText folder and extract the derivatives of root words\n",
    "'''\n",
    "\n",
    "#The Function stop_words was written by Kumbula\n",
    "filtered_words = []\n",
    "def stop_words(tokens):\n",
    "    \"\"\"\n",
    "    This function is for removing the stopwords from the text file.\n",
    "    Filename: Name of the file\n",
    "    return: filtered_words\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))  # Creating a List of stopwords\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)  # Add all words after stopwords have been removed\n",
    "    # print (\"Filtered_words:\",filtered_words )\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "# Go through each statement and find words that are derivatives of root words\n",
    "current_dir = os.getcwd()\n",
    "textdir = (r\"C:\\Users\\manalais\\decode-diversity-statements\\tokenText\")\n",
    "for filename in os.listdir(textdir)[:]:\n",
    "    filepath = os.path.join(textdir, filename)\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        tokens = tokenizer.tokenize(text)  # Returns a text as a list of words with punctuations\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()]  # Changes all the words into lower case        \n",
    "        for root in clean_words:\n",
    "            for leaf in tokens:\n",
    "                similiarity = difflib.SequenceMatcher(None, root, leaf).ratio()\n",
    "                if similiarity > 0.90:\n",
    "                    if leaf not in dic[root]:   #only append to dictionary if the value doesn't already exist\n",
    "                        dic[root].append(leaf)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(Manual work): The difflib.SequenceMatcher matches some words that are similiar but not leaves of the root word.\n",
    "For instance, pain --> paint. Therefore, I am removing them from the dictionary values manually '''\n",
    "\n",
    "# for value in dic.values():\n",
    "#     for i in value:\n",
    "#         if i == 'paint':\n",
    "#             value.remove(i)\n",
    "#         if i == 'posit':\n",
    "#             value.remove(i)\n",
    "#         if i == 'present':\n",
    "#             value.remove(i)\n",
    "#         if i == 'los':\n",
    "#             value.remove(i)\n",
    "#         if i == 'pat':\n",
    "#             value.remove(i)\n",
    "#         if i == 'sin':\n",
    "#             value.remove(i)\n",
    "#         if i == 'lack':\n",
    "#             value.remove(i)\n",
    "#         if i == 'back':\n",
    "#             value.remove(i)\n",
    "#         if i == 'far':\n",
    "#             value.remove(i)\n",
    "#         if i == 'ears':\n",
    "#             value.remove(i)\n",
    "#         if i == 'inclusivevt':\n",
    "#             value.remove(i)\n",
    "#         if i == 'fraction':\n",
    "#             value.remove(i)\n",
    "#         if i == 'reaction':\n",
    "#             value.remove(i)\n",
    "#         if i == 'emory':\n",
    "#             value.remove(i)\n",
    "#         if i == 'fore':\n",
    "#             value.remove(i) \n",
    "#         if i == 'bright':\n",
    "#             value.remove(i)\n",
    "#         if i == 'wright':\n",
    "#             value.remove(i)\n",
    "#         if i == 'identify':\n",
    "#             value.remove(i)\n",
    "#         if i == 'grace':\n",
    "#             value.remove(i)\n",
    "#         if i == 'chase':\n",
    "#             value.remove(i)\n",
    "#         if i == 'ceases':\n",
    "#             value.remove(i)\n",
    "#         if i == 'causes':\n",
    "#             value.remove(i) \n",
    "#         if i == 'vice':\n",
    "#             value.remove(i)\n",
    "#         if i == 'danger':\n",
    "#             value.remove(i)\n",
    "#         if i == 'treat':\n",
    "#             value.remove(i)\n",
    "#         if i == 'quality':\n",
    "#             value.remove(i)\n",
    "#         if i == 'ear':\n",
    "#             value.remove(i)\n",
    "#         if i == 'cease':\n",
    "#             value.remove(i)\n",
    "#         if i == 'cause':\n",
    "#             value.remove(i)\n",
    "#         if i == 'movement':\n",
    "#             value.remove(i)\n",
    "#         if i == 'protest':\n",
    "#             value.remove(i)\n",
    "#         if i == 'justice':\n",
    "#             value.remove(i)\n",
    "#         if i == 'nondiscrimination':\n",
    "#             value.remove(i)\n",
    "#         if i == 'america':\n",
    "#             value.remove(i)\n",
    "#         if i == 'inaction':\n",
    "#             value.remove(i)\n",
    "#         if i == 'knee':\n",
    "#             value.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' (Manual work): The dictionary contains key:value errors that needs to be fixed'''\n",
    "# dic['equality'] = [ 'equality']\n",
    "# dic['equality']\n",
    "# dic['inequity'] = ['inequity']\n",
    "# dic['inequity']\n",
    "# dic['officer'] = ['officer', 'officers']\n",
    "# dic['officer']\n",
    "# dic['race'] = ['race', 'races', 'racism', 'racist']\n",
    "# dic['race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Visualize the data'''\n",
    "\n",
    "key_list = []\n",
    "value_list = []\n",
    "\n",
    "\n",
    "for k in dic.keys():\n",
    "    key_list.append(k)\n",
    "for v in dic.values():\n",
    "    value_list.append(v)\n",
    "\n",
    "print(\"\\n\" + \"\\033[1m\" + \"Root and Leaf Plot\".center(60) + \"\\n\")\n",
    "for key,value,count in zip(key_list,value_list,word_count):\n",
    "    print(\"\\033[1m\" + key + \"\\033[0m\" + '| ' + \" \".join(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
