{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lemmatizes words\n",
    "# #Task: lemmatizeWords function needs to be improved to get root of ALL words\n",
    "# def lemmatizeWords(sentence):\n",
    "#     '''takes as an input tokenized sentence without punctuations. Returns lemmatized words'''\n",
    "    \n",
    "#     wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_words = []\n",
    "#     sentence = nltk.pos_tag(sentence)   #find POS in the sentence\n",
    "#     for word, tag in sentence:\n",
    "#         if tag.startswith(\"NN\"):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "#         elif tag.startswith('VB'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "#         elif tag.startswith('JJ'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='a'))\n",
    "#         else:\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "#     return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(r'C:\\Users\\manalais\\Box\\Web Developer\\Book1.csv')\n",
    "# line = file.readlines()\n",
    "# #print(line)\n",
    "# new_file = open(r'C:\\Users\\manalais\\Box\\Web Developer\\root_words.txt',mode=\"a+\",encoding=\"utf-8\") \n",
    "\n",
    "# # string = \"HEL\"\n",
    "# # print(string.lower())\n",
    "# # start = word.index('\"')\n",
    "# root_list = []\n",
    "# words = []\n",
    "# # end = word.index(' ')\n",
    "# for word in line:\n",
    "#     if word.find('\"') or word.find('->'):\n",
    "#         word = word.replace('\"', '') \n",
    "#         word = word.replace('->', '')\n",
    "#         word = word.replace('ï»¿', '')  \n",
    "#         root_list.append(word)\n",
    "# for i in root_list:\n",
    "#     i = i.lower()\n",
    "#     words.append((i[0:i.index(' ')]))\n",
    "    \n",
    "    \n",
    "# # words = '\\n '.join(words)\n",
    "# # new_file.writelines(words)\n",
    "# # new_file.close()\n",
    "\n",
    "\n",
    "# #lemmatize it\n",
    "# # run it through this\n",
    "# from nltk.stem import LancasterStemmer\n",
    "# lancaster=LancasterStemmer()\n",
    "# #porter = PorterStemmer()\n",
    "# stem_list = []\n",
    "# root_words = []\n",
    "# compound_words = [\"happiness\", \"Community\", \"collective\", \"distribution\", \"collection\", \"successfully\", 'impacted']\n",
    "# for word in compound_words:\n",
    "#     stem_list.append(lancaster.stem(word))\n",
    "# print(stem_list)\n",
    "\n",
    "# for c in stem_list:\n",
    "#     for w in words:\n",
    "#         similiarity = difflib.SequenceMatcher(None, c, w).ratio()\n",
    "#         if similiarity > 0.91:\n",
    "#             root_words.append(w)\n",
    "\n",
    "# print(root_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import LancasterStemmer\n",
    "# lancaster=LancasterStemmer()\n",
    "# porter = PorterStemmer()\n",
    "# stem_list = []\n",
    "# root_words = []\n",
    "# test = [\"happiness\"]\n",
    "# # word_list = [\"happy\", \"race\", \"collect\", \"arrive\", \"distribute\"]\n",
    "# # compound_words = [\"happiness\", \"racism\", \"collective\", \"arrival\", \"distribution\", \"collection\"]\n",
    "# # for word in compound_words:\n",
    "# #     stem_list.append(lancaster.stem(word))\n",
    "# # print(stem_list)\n",
    "    \n",
    "# for c in stem_list:\n",
    "#     #print(root)\n",
    "#     for w in word_list:\n",
    "#         #print(leaf)\n",
    "#         similiarity = difflib.SequenceMatcher(None, c, w).ratio()\n",
    "#         if similiarity > 0.80:\n",
    "#             root_words.append(w)\n",
    "# print(root_words)\n",
    "\n",
    "# from nltk.corpus import words\n",
    "# word_list = words.words()\n",
    "\n",
    "# for word in test:\n",
    "#     stem_list.append(lancaster.stem(word))\n",
    "# print(stem_list)\n",
    "\n",
    "\n",
    "# for t in test:\n",
    "#     #print(root)\n",
    "#     for w in word_list:\n",
    "#         #print(leaf)\n",
    "#         similiarity = difflib.SequenceMatcher(None, t, w).ratio()\n",
    "#         if similiarity > 0.80:\n",
    "#             if w[-3:] != \"ness\":\n",
    "#                 root_words.append(w)\n",
    "# print(root_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# sn = SnowballStemmer(\"english\")\n",
    "\n",
    "# stem = sn.stem(\"happiness\")\n",
    "# print(lemma(stem))\n",
    "# print(lemmatizer(stem,wordnet.VERB ))\n",
    "# print(sn.stem(\"happy\"))\n",
    "# print(lemmatizeWords(stem))\n",
    "# # What I want to do:\n",
    "\n",
    "# #print(some_unstem_function(\"happi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.utils import lemmatize\n",
    "# sentence = \"The striped bats were hanging on their feet and ate best fishes\"\n",
    "# lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]\n",
    "# #> ['striped', 'bat', 'be', 'hang', 'foot', 'eat', 'best', 'fish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Wordnet lemmatization functions\n",
    "# def lemmatizeWords(sentence):\n",
    "#     '''takes as an input tokenized sentence without punctuations. Returns lemmatized words'''\n",
    "#     wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_words = []\n",
    "#     sentence = nltk.pos_tag(sentence)   #find POS in the sentence\n",
    "#     print(sentence)\n",
    "#     for word, tag in sentence:\n",
    "#         if tag.startswith(\"NN\"):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "#         elif tag.startswith('VB'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "#         elif tag.startswith('JJ'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='a'))\n",
    "#         elif tag.startswith('RB'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='r'))\n",
    "#         else:\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "#     return lemmatized_words\n",
    "\n",
    "\n",
    "# #print(word_list)\n",
    "# #print(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "# sentence = \"The striped further are impacted on their feet for best principles organization happiness\"\n",
    "# word_list = nltk.word_tokenize(sentence)\n",
    "# print(lemmatizeWords(word_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def lemmatizer(word, pos):\n",
    "#     lemma = wordnet._morphy(word, pos)\n",
    "#     if lemma:\n",
    "#         return min(lemma, key=len)\n",
    "#     else:\n",
    "#         return word\n",
    "# #print(lemmatizer(\"scientist\",wordnet.NOUN ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_wordnet_pos(word):\n",
    "#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.VERB)\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# #print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Wordnet lemmatization functions\n",
    "# def lemmatizeWords(sentence):\n",
    "#     '''takes as an input tokenized sentence without punctuations. Returns lemmatized words'''\n",
    "    \n",
    "#     wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_words = []\n",
    "#     sentence = nltk.pos_tag(sentence)   #find POS in the sentence\n",
    "#     for word, tag in sentence:\n",
    "#         if tag.startswith(\"NN\"):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "#         elif tag.startswith('VB'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "#         elif tag.startswith('JJ'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='a'))\n",
    "#         else:\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "#     return lemmatized_words\n",
    "\n",
    "# sentence = \"The striped further are impacted on their feet for best\"\n",
    "# word_list = nltk.word_tokenize(sentence)\n",
    "# #print(word_list)\n",
    "# print(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "# def get_wordnet_pos(word):\n",
    "#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "# print(lemmatizeWords(word_list))\n",
    "\n",
    "# def lemmatizer(word, pos):\n",
    "#     lemma = wordnet._morphy(word, pos)\n",
    "#     if lemma:\n",
    "#         return min(lemma, key=len)\n",
    "#     else:\n",
    "#         return word\n",
    "# print(lemmatizer(\"happi\",wordnet.NOUN ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.utils import lemmatize\n",
    "# sentence = \"The striped bats were hanging on their feet and ate best fishes\"\n",
    "# lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]\n",
    "# #> ['striped', 'bat', 'be', 'hang', 'foot', 'eat', 'best', 'fish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# sn = SnowballStemmer(\"english\")\n",
    "\n",
    "# stem = sn.stem(\"happiness\")\n",
    "# print(lemma(stem))\n",
    "# print(lemmatizer(stem,wordnet.VERB ))\n",
    "# print(sn.stem(\"happy\"))\n",
    "# print(lemmatizeWords(stem))\n",
    "# # What I want to do:\n",
    "\n",
    "# #print(some_unstem_function(\"happi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pip install -e word_forms.\n",
    "\n",
    "# from word_forms.word_forms import get_word_forms\n",
    "# get_word_forms('conclusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import LancasterStemmer\n",
    "# lancaster=LancasterStemmer()\n",
    "# porter = PorterStemmer()\n",
    "# stem_list = []\n",
    "# root_words = []\n",
    "# test = [\"happiness\"]\n",
    "# # word_list = [\"happy\", \"race\", \"collect\", \"arrive\", \"distribute\"]\n",
    "# # compound_words = [\"happiness\", \"racism\", \"collective\", \"arrival\", \"distribution\", \"collection\"]\n",
    "# # for word in compound_words:\n",
    "# #     stem_list.append(lancaster.stem(word))\n",
    "# # print(stem_list)\n",
    "    \n",
    "# for c in stem_list:\n",
    "#     #print(root)\n",
    "#     for w in word_list:\n",
    "#         #print(leaf)\n",
    "#         similiarity = difflib.SequenceMatcher(None, c, w).ratio()\n",
    "#         if similiarity > 0.80:\n",
    "#             root_words.append(w)\n",
    "# print(root_words)\n",
    "\n",
    "# from nltk.corpus import words\n",
    "# word_list = words.words()\n",
    "\n",
    "# for word in test:\n",
    "#     stem_list.append(lancaster.stem(word))\n",
    "# print(stem_list)\n",
    "\n",
    "\n",
    "# for t in test:\n",
    "#     #print(root)\n",
    "#     for w in word_list:\n",
    "#         #print(leaf)\n",
    "#         similiarity = difflib.SequenceMatcher(None, t, w).ratio()\n",
    "#         if similiarity > 0.80:\n",
    "#             if w[-3:] != \"ness\":\n",
    "#                 root_words.append(w)\n",
    "# print(root_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(r'C:\\Users\\manalais\\Box\\Web Developer\\Book1.csv')\n",
    "# line = file.readlines()\n",
    "# #print(line)\n",
    "# new_file = open(r'C:\\Users\\manalais\\Box\\Web Developer\\root_words.txt',mode=\"a+\",encoding=\"utf-8\") \n",
    "\n",
    "# # string = \"HEL\"\n",
    "# # print(string.lower())\n",
    "# # start = word.index('\"')\n",
    "# root_list = []\n",
    "# words = []\n",
    "# # end = word.index(' ')\n",
    "# for word in line:\n",
    "#     if word.find('\"') or word.find('->'):\n",
    "#         word = word.replace('\"', '') \n",
    "#         word = word.replace('->', '')\n",
    "#         word = word.replace('ï»¿', '')  \n",
    "#         root_list.append(word)\n",
    "# for i in root_list:\n",
    "#     i = i.lower()\n",
    "#     words.append((i[0:i.index(' ')]))\n",
    "    \n",
    "    \n",
    "# words = '\\n '.join(words)\n",
    "# new_file.writelines(words)\n",
    "# new_file.close()\n",
    "\n",
    "\n",
    "#lemmatize it\n",
    "# run it through this\n",
    "# from nltk.stem import LancasterStemmer\n",
    "# lancaster=LancasterStemmer()\n",
    "# #porter = PorterStemmer()\n",
    "# stem_list = []\n",
    "# root_words = []\n",
    "# compound_words = [\"happiness\", \"Community\", \"collective\", \"distribution\", \"collection\", \"successfully\", 'impacted']\n",
    "# for word in compound_words:\n",
    "#     stem_list.append(lancaster.stem(word))\n",
    "# print(stem_list)\n",
    "\n",
    "# for c in stem_list:\n",
    "#     for w in words:\n",
    "#         similiarity = difflib.SequenceMatcher(None, c, w).ratio()\n",
    "#         if similiarity > 0.91:\n",
    "#             root_words.append(w)\n",
    "\n",
    "# print(root_words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''(Optional) Extra step to get the base words'''\n",
    "\n",
    "##############################################################################################################################\n",
    "# '''(Optional Method) Extra step to get the base words'''\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# clean_words = []\n",
    "# porter = PorterStemmer()\n",
    "# for words, tag in tagged_tokens:\n",
    "#     if  not tag.startswith('NNP' or 'NNPS'): # remove propr nouns\n",
    "#         if tag.startswith('NN' or 'NNS'):\n",
    "#             clean_words.append(porter.stem(word))\n",
    "#         elif tag.startswith('VB' or 'VBP' or 'VBD' or 'VBG' or 'VBP' or 'VBZ'):\n",
    "#             clean_words.append(lemmatizer.lemmatize(words, pos='v'))\n",
    "#         elif tag.startswith('JJ' or 'JJR' or 'JJS' ):\n",
    "#             clean_words.append(lemmatizer.lemmatize(words, pos='a'))\n",
    "#         elif tag.startswith('RB' or 'RBR' or 'RBS' or 'RP'):\n",
    "#             clean_words.append(lemmatizer.lemmatize(words, pos='r'))\n",
    "#         else:\n",
    "#             clean_words.append(lemmatizer.lemmatize(words))\n",
    "\n",
    "# print(clean_words)   \n",
    "#############################################################################################################################\n",
    "# '''(Optional Method) Extra step to get the base words'''\n",
    "# # iterates through the lemma words list to unstem the words to their original form\n",
    "# root_words = []\n",
    "# for c in clean_words:\n",
    "#     for w in lemma_words:\n",
    "#         similiarity = difflib.SequenceMatcher(None, c, w).ratio()\n",
    "#         if similiarity > 0.91:\n",
    "#             root_words.append(w)\n",
    "# print('After unstemming: ')\n",
    "# print(root_words)\n",
    "##############################################################################################################################\n",
    "# ''' Create a leaf words list and append all of the leaf words to the dictionary'''\n",
    "\n",
    "# #create a dictionory here assign all the roots to keys and empty list as values\n",
    "# dic = {}\n",
    "# for root in clean_words:\n",
    "#     dic[root] = []\n",
    "    \n",
    "\n",
    "\n",
    "# leaf_list = list(set(leaf_list))  # removes the duplicate words\n",
    "\n",
    "# # This code finds the leaf words and returns it as a list   \n",
    "# for root in clean_words:\n",
    "#     #print(root)\n",
    "#     for leaf in leaf_list:\n",
    "#         #print(leaf)\n",
    "#         similiarity = difflib.SequenceMatcher(None, root, leaf).ratio()\n",
    "#         if similiarity > 0.7:\n",
    "#             dic[root].append(leaf)\n",
    "# print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lemmatizes words\n",
    "# #Task: lemmatizeWords function needs to be improved to get root of ALL words\n",
    "# def lemmatizeWords(sentence):\n",
    "#     '''takes as an input tokenized sentence without punctuations. Returns lemmatized words'''  \n",
    "#     wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_words = []\n",
    "#     sentence = nltk.pos_tag(sentence)   #find POS in the sentence\n",
    "#     for word, tag in sentence:\n",
    "#         if tag.startswith(\"NN\"):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "#         elif tag.startswith('VB'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "#         elif tag.startswith('JJ'):\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='a'))\n",
    "#         else:\n",
    "#             lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "#     return lemmatized_words\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
