{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manalais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "def removePunctuation(sentence):\n",
    "    '''Input must be string. Tokenizes and removes punctuation.'''\n",
    "    \n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    punctuations=\"\\\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~–\"\n",
    "    for word in tokenized_sentence:\n",
    "        if word in punctuations:\n",
    "            tokenized_sentence.remove(word)\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove StopWords\n",
    "def removeStopWords(sentence):\n",
    "    '''takes tokenized words as an input and removes all the stop words'''\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in stopwords.words('english'):\n",
    "            sentence.remove(word)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizes words\n",
    "#Task: lemmatizeWords function needs to be improved to get root of ALL words\n",
    "def lemmatizeWords(sentence):\n",
    "    '''takes as an input tokenized sentence without punctuations. Returns lemmatized words'''\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    sentence = nltk.pos_tag(sentence)   #find POS in the sentence\n",
    "    for word, tag in sentence:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB'):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('JJ'):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='a'))\n",
    "        else:\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': ['one'],\n",
       " 'scientific': ['science', 'science', 'science', 'scientists', 'science'],\n",
       " 'let': [],\n",
       " 'critical': ['critical', 'critical'],\n",
       " 'advocacy': ['advocates'],\n",
       " 'beyond': [],\n",
       " 'to': ['to'],\n",
       " 'harrowing': [],\n",
       " 'world': ['world'],\n",
       " 'understandable': [],\n",
       " 'more': [],\n",
       " 'society': ['social', 'society'],\n",
       " 'future': [],\n",
       " 'team': [],\n",
       " 'helplessness': [],\n",
       " 'we': ['we', 'we'],\n",
       " 'answer': [],\n",
       " 'leadership': ['leadership'],\n",
       " 'sadness': [],\n",
       " 'week': [],\n",
       " 'program': ['programs'],\n",
       " 'science': ['science', 'science', 'science', 'scientists', 'science'],\n",
       " 'unchecked': [],\n",
       " 'friend': [],\n",
       " 'confident': [],\n",
       " 'troubled': [],\n",
       " 'recommit': ['recommit'],\n",
       " 'feel': ['feel'],\n",
       " 'ensure': [],\n",
       " 'upon': [],\n",
       " 'advance': [],\n",
       " 'torrent': [],\n",
       " 'change': ['change'],\n",
       " 'must': ['must'],\n",
       " 'inclusion': ['inclusion'],\n",
       " 'unbalanced': [],\n",
       " 'emotion': [],\n",
       " 'provide': [],\n",
       " 'happen': [],\n",
       " 'navigate': [],\n",
       " 'generation': [],\n",
       " 'the': ['the', 'the'],\n",
       " 'possible': [],\n",
       " 'serve': [],\n",
       " 'publication': [],\n",
       " 'problem': [],\n",
       " 'enter': ['enterprise'],\n",
       " 'equity': ['equity'],\n",
       " 'be': ['be'],\n",
       " 'around': [],\n",
       " 'foundation': [],\n",
       " 'fabric': [],\n",
       " 'destroy': [],\n",
       " 'time': ['time', 'time'],\n",
       " 'community': ['community',\n",
       "  'community',\n",
       "  'community',\n",
       "  'communities',\n",
       "  'community',\n",
       "  'community',\n",
       "  'communities'],\n",
       " 'axis': [],\n",
       " 'discrimination': [],\n",
       " 'seem': [],\n",
       " 'intervention': [],\n",
       " 'ever': ['everyone', 'ever'],\n",
       " 'principle': ['principles', 'principles'],\n",
       " '’': [],\n",
       " 'rededicate': [],\n",
       " 'of': ['of', 'of'],\n",
       " 'carry': [],\n",
       " 'this': ['this'],\n",
       " 'last': ['last'],\n",
       " 'lack': [],\n",
       " 'tear': [],\n",
       " 'together': ['together', 'together'],\n",
       " 'require': [],\n",
       " 'make': [],\n",
       " 'deeply': [],\n",
       " 'unprecedented': [],\n",
       " 'advocate': ['advocates'],\n",
       " 'go': [],\n",
       " 'scientist': ['science', 'science', 'science', 'scientists', 'science'],\n",
       " 'anger': [],\n",
       " 'begin': [],\n",
       " 'in': ['in'],\n",
       " 'fear': [],\n",
       " 'safe': [],\n",
       " 'face': [],\n",
       " 'impacted': [],\n",
       " 'event': [],\n",
       " 'hope': [],\n",
       " 'can': ['can'],\n",
       " 'define': [],\n",
       " 'no': [],\n",
       " 'support': ['support', 'support'],\n",
       " 'build': [],\n",
       " 'systemic': [],\n",
       " 'diversity': [],\n",
       " 'wherever': [],\n",
       " 'social': ['social', 'society'],\n",
       " 'national': [],\n",
       " 'steady': [],\n",
       " 'past': [],\n",
       " 'escape': [],\n",
       " 'colleague': [],\n",
       " 'respect': ['respected'],\n",
       " 'trust': [],\n",
       " 'everyone': ['everyone', 'ever'],\n",
       " 'injustice': [],\n",
       " 'our': ['our', 'our', 'our', 'our', 'our', 'our'],\n",
       " 'need': [],\n",
       " 'not': ['not'],\n",
       " 'every': ['everyone', 'ever'],\n",
       " 'principles': ['principles', 'principles'],\n",
       " 'bare': [],\n",
       " 'unwavering': [],\n",
       " 'communities': ['community',\n",
       "  'community',\n",
       "  'community',\n",
       "  'communities',\n",
       "  'community',\n",
       "  'community',\n",
       "  'communities'],\n",
       " 'challenge': ['challenges'],\n",
       " 'these': ['these'],\n",
       " 'pull': [],\n",
       " 'than': [],\n",
       " 'progress': ['programs'],\n",
       " 'along': [],\n",
       " 'place': [],\n",
       " 'another': ['another'],\n",
       " 'start': [],\n",
       " 'enterprise': ['enterprise'],\n",
       " 'successfully': ['successfully'],\n",
       " 'mission': [],\n",
       " 'ability': [],\n",
       " 'pain': [],\n",
       " 'a': ['a', 'a', 'a'],\n",
       " 'success': ['successfully'],\n",
       " 'month': [],\n",
       " 'force': ['force'],\n",
       " 'good': [],\n",
       " 'uncertainty': [],\n",
       " 'racism': [],\n",
       " 'unleash': [],\n",
       " 'organization': ['organization'],\n",
       " 'relevant': [],\n",
       " 'bring': [],\n",
       " 'sexism': [],\n",
       " 'homophobia': [],\n",
       " 'come': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will be the main() function\n",
    "# Comments with task means the code needs improvements\n",
    "\n",
    "dic = dict()\n",
    "root_list = []\n",
    "leaf_list = []\n",
    "\n",
    "# This code finds the root words and returns it as a list\n",
    "my_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\decode-diversity-statements\\texts\\AAAS_(American_Association_for_the_Advancement_of_Science).txt\", encoding=\"utf-8\")\n",
    "for line in my_file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    rem_ove = removePunctuation(line)\n",
    "    noStop = removeStopWords(rem_ove)\n",
    "    lemmaWord = lemmatizeWords(noStop)\n",
    "    #Task: add the lemmatized words with a loop so it doesn't create a list of list\n",
    "    root_list.append(lemmaWord)\n",
    "\n",
    "\n",
    "flattened_root = []   #flattening out list of list to make one list\n",
    "for sublist in root_list: \n",
    "    for val in sublist:\n",
    "        flattened_root.append(val)\n",
    "\n",
    "flattened_root = list(set(flattened_root))  # removes the duplicate words \n",
    "\n",
    "# print(\"This is the root words\" + \"\\n\")\n",
    "# print(flattened_root)\n",
    "\n",
    "#task: create a dictionory here assign all the roots to keys and empty list as values\n",
    "#Create a dictionary where each root word corresponds to leaves \n",
    "# Task: use something like python string comparison to compare words\n",
    "\n",
    "\n",
    "\n",
    "# This code finds the leaf words and returns it as a list\n",
    "# Task: use the token text version for this part\n",
    "my_file2 = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\decode-diversity-statements\\texts\\AAAS_(American_Association_for_the_Advancement_of_Science).txt\", encoding=\"utf-8\")\n",
    "for li in my_file2:\n",
    "    rem_ove = removePunctuation(li)\n",
    "    noStop = removeStopWords(rem_ove)\n",
    "    leaf_list.append(noStop)\n",
    "#print(leaf_list)\n",
    "\n",
    "flattened_leaf = []  ##flattening out list of list to make one list\n",
    "for sublist in leaf_list:\n",
    "    for val in sublist:\n",
    "        flattened_leaf.append(val)\n",
    "# print(\"\\n\" + \"This is the leaf words\" + \"\\n\")\n",
    "# print(flattened_leaf)\n",
    "\n",
    "\n",
    "\n",
    "for root in flattened_root:\n",
    "    for leaf in flattened_leaf:\n",
    "        if root[:4] == leaf[:4]:\n",
    "            if root in dic:\n",
    "                dic[root].append(leaf)\n",
    "            else:\n",
    "                   dic[root] = []\n",
    "                \n",
    "            \n",
    "dic\n",
    "\n",
    "#Task: visualization: print out the roots and leaves\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the unused code\n",
    "\n",
    "\n",
    "\n",
    "# word_count = dict()\n",
    "\n",
    "# for word in flattened:\n",
    "#     if word in word_count:\n",
    "#         word_count[word] = word_count[word]+1\n",
    "#     else:\n",
    "#         word_count[word] = 1\n",
    "\n",
    "# #Print the contents of dictionary\n",
    "# for key in list(word_count.keys()):\n",
    "#     #resultsfile.write(key + \":\" + str(d[key]) +'\\n')\n",
    "#     print(key, \":\", word_count[key])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# Counter = Counter(flattened)\n",
    "# most_occur = Counter.most_common(4)\n",
    "\n",
    "\n",
    "\n",
    "# for listitems in flattened:\n",
    "#     if listitem not in flattened:\n",
    "#         dic[listitem]\n",
    "        \n",
    "        \n",
    "\n",
    "# new_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\Stemming and Lemmatization\\newfile.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "# for listitem in flattened:\n",
    "#     print(listitem)\n",
    "#     new_file.write(\"%s\\n\" %listitem)\n",
    "\n",
    "\n",
    "\n",
    "# keys = dic.keys()\n",
    "# for i in keys:\n",
    "#     new_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\Stemming and Lemmatization\\newfile.txt\",mode=\"a\",encoding=\"utf-8\")\n",
    "#     new_file.write(\"%s\\n\" %i)\n",
    "#     #new_file.writelines(keys)\n",
    "# new_file.close()\n",
    "# #print(keys)\n",
    "# values = dic.values()\n",
    "# for i in values:\n",
    "#     new_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\Stemming and Lemmatization\\newfile2.txt\",mode=\"a\",encoding=\"utf-8\")\n",
    "#     new_file.writelines(\"%s\\n\" %i)\n",
    "#     #new_file.writelines(keys)\n",
    "# new_file.close()\n",
    "#print(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
