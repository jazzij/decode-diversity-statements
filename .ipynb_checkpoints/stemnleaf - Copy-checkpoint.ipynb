{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manalais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\manalais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "import difflib\n",
    "import pandas as pd\n",
    "nltk.download('words')\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "def removePunctuation(sentence):\n",
    "    '''Input must be string. Tokenizes and removes punctuation.'''\n",
    "    \n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    punctuations=\"\\\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~–\"\n",
    "    for word in tokenized_sentence:\n",
    "        if word in punctuations:\n",
    "            tokenized_sentence.remove(word)\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove StopWords\n",
    "def removeStopWords(sentence):\n",
    "    '''takes tokenized words as an input and removes all the stop words'''\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in stopwords.words('english'):\n",
    "            sentence.remove(word)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizes words\n",
    "#Task: lemmatizeWords function needs to be improved to get root of ALL words\n",
    "def lemmatizeWords(sentence):\n",
    "    '''takes as an input tokenized sentence without punctuations. Returns lemmatized words'''\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    sentence = nltk.pos_tag(sentence)   #find POS in the sentence\n",
    "    for word, tag in sentence:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB'):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('JJ'):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='a'))\n",
    "        else:\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                     Root and Leaf Plot                     \n",
      "\n",
      "\u001b[1mhappen\u001b[0m | \n",
      "\u001b[1mbare\u001b[0m | bare are\n",
      "\u001b[1mface\u001b[0m | face\n",
      "\u001b[1mintervention\u001b[0m | intervention\n",
      "\u001b[1mhope\u001b[0m | hope\n",
      "\u001b[1mrespect\u001b[0m | respect respected\n",
      "\u001b[1mscience\u001b[0m | science\n",
      "\u001b[1mleadership\u001b[0m | leadership\n",
      "\u001b[1mimpacted\u001b[0m | impacted\n",
      "\u001b[1munchecked\u001b[0m | unchecked\n",
      "\u001b[1mgood\u001b[0m | good\n",
      "\u001b[1mseem\u001b[0m | seems\n",
      "\u001b[1mcommunity\u001b[0m | community\n",
      "\u001b[1msuccess\u001b[0m | successes\n",
      "\u001b[1manswer\u001b[0m | answers\n",
      "\u001b[1mdiscrimination\u001b[0m | discrimination\n",
      "\u001b[1mweek\u001b[0m | weeks\n",
      "\u001b[1msuccessfully\u001b[0m | successfully\n",
      "\u001b[1mrededicate\u001b[0m | rededicate\n",
      "\u001b[1mtear\u001b[0m | tear\n",
      "\u001b[1mthese\u001b[0m | these\n",
      "\u001b[1mtime\u001b[0m | times time\n",
      "\u001b[1manother\u001b[0m | another\n",
      "\u001b[1mworld\u001b[0m | world\n",
      "\u001b[1mdestroy\u001b[0m | destroy\n",
      "\u001b[1mdiversity\u001b[0m | diversity\n",
      "\u001b[1mbring\u001b[0m | bring\n",
      "\u001b[1mnevertheless\u001b[0m | nevertheless\n",
      "\u001b[1mleft\u001b[0m | left let\n",
      "\u001b[1mmission\u001b[0m | mission\n",
      "\u001b[1mfriend\u001b[0m | friends\n",
      "\u001b[1mchallenge\u001b[0m | challenges\n",
      "\u001b[1mupon\u001b[0m | upon\n",
      "\u001b[1mprinciples\u001b[0m | principles\n",
      "\u001b[1mability\u001b[0m | ability\n",
      "\u001b[1min\u001b[0m | in\n",
      "\u001b[1mracism\u001b[0m | racism\n",
      "\u001b[1mfear\u001b[0m | fear\n",
      "\u001b[1mcritical\u001b[0m | critical\n",
      "\u001b[1mforce\u001b[0m | force\n",
      "\u001b[1maaas\u001b[0m | aaas\n",
      "\u001b[1memotion\u001b[0m | emotion\n",
      "\u001b[1mpain\u001b[0m | pain\n",
      "\u001b[1mmonth\u001b[0m | months\n",
      "\u001b[1msteady\u001b[0m | steady\n",
      "\u001b[1mtorrent\u001b[0m | torrent\n",
      "\u001b[1mensure\u001b[0m | \n",
      "\u001b[1mmany\u001b[0m | many\n",
      "\u001b[1mevent\u001b[0m | events\n",
      "\u001b[1muncertainty\u001b[0m | uncertainty\n",
      "\u001b[1maxis\u001b[0m | axis\n",
      "\u001b[1mlack\u001b[0m | lack\n",
      "\u001b[1mu\u001b[0m | \n",
      "\u001b[1ma\u001b[0m | a\n",
      "\u001b[1munprecedented\u001b[0m | unprecedented\n",
      "\u001b[1m’\u001b[0m | ’\n",
      "\u001b[1mbegin\u001b[0m | begins\n",
      "\u001b[1mmore\u001b[0m | more\n",
      "\u001b[1mteam\u001b[0m | team\n",
      "\u001b[1mcolleague\u001b[0m | colleagues\n",
      "\u001b[1mcan\u001b[0m | can\n",
      "\u001b[1mtogether\u001b[0m | together\n",
      "\u001b[1mnot\u001b[0m | not\n",
      "\u001b[1mno\u001b[0m | no\n",
      "\u001b[1msociety\u001b[0m | society\n",
      "\u001b[1mchange\u001b[0m | change\n",
      "\u001b[1malong\u001b[0m | along\n",
      "\u001b[1mone\u001b[0m | one\n",
      "\u001b[1mprogress\u001b[0m | progress\n",
      "\u001b[1msafe\u001b[0m | safe\n",
      "\u001b[1mhomophobia\u001b[0m | homophobia\n",
      "\u001b[1mdeeply\u001b[0m | deeply\n",
      "\u001b[1mfeel\u001b[0m | feel\n",
      "\u001b[1mrecommit\u001b[0m | recommit\n",
      "\u001b[1mhave\u001b[0m | \n",
      "\u001b[1mtroubled\u001b[0m | troubled\n",
      "\u001b[1mnational\u001b[0m | national\n",
      "\u001b[1morganization\u001b[0m | organization\n",
      "\u001b[1menterprise\u001b[0m | enterprise\n",
      "\u001b[1mnavigate\u001b[0m | navigate\n",
      "\u001b[1mscientific\u001b[0m | scientific\n",
      "\u001b[1maround\u001b[0m | around\n",
      "\u001b[1mrelevant\u001b[0m | relevant\n",
      "\u001b[1mstart\u001b[0m | starts\n",
      "\u001b[1mevery\u001b[0m | every ever\n",
      "\u001b[1mpull\u001b[0m | pull\n",
      "\u001b[1mfuture\u001b[0m | future\n",
      "\u001b[1munbalanced\u001b[0m | unbalanced\n",
      "\u001b[1mever\u001b[0m | every ever\n",
      "\u001b[1madvance\u001b[0m | \n",
      "\u001b[1mfoundation\u001b[0m | foundations\n",
      "\u001b[1mpast\u001b[0m | past\n",
      "\u001b[1mconfident\u001b[0m | confident\n",
      "\u001b[1mall\u001b[0m | all\n",
      "\u001b[1mthe\u001b[0m | the\n",
      "\u001b[1msexism\u001b[0m | sexism\n",
      "\u001b[1mhelplessness\u001b[0m | helplessness\n",
      "\u001b[1meveryone\u001b[0m | everyone\n",
      "\u001b[1mlast\u001b[0m | last\n",
      "\u001b[1mthis\u001b[0m | this\n",
      "\u001b[1madvocate\u001b[0m | advocates\n",
      "\u001b[1madvocacy\u001b[0m | advocacy\n",
      "\u001b[1menter\u001b[0m | enter\n",
      "\u001b[1mcommunities\u001b[0m | communities\n",
      "\u001b[1mcarry\u001b[0m | carry\n",
      "\u001b[1mprinciple\u001b[0m | principles\n",
      "\u001b[1msupport\u001b[0m | support\n",
      "\u001b[1mprovide\u001b[0m | provide\n",
      "\u001b[1mwe\u001b[0m | we\n",
      "\u001b[1mhatred\u001b[0m | hatred\n",
      "\u001b[1mgo\u001b[0m | go\n",
      "\u001b[1mneed\u001b[0m | needs\n",
      "\u001b[1mcome\u001b[0m | come\n",
      "\u001b[1mgeneration\u001b[0m | generation\n",
      "\u001b[1mpublication\u001b[0m | publications\n",
      "\u001b[1munderstandable\u001b[0m | understandable\n",
      "\u001b[1minjustice\u001b[0m | injustice\n",
      "\u001b[1mbeyond\u001b[0m | beyond\n",
      "\u001b[1manger\u001b[0m | anger\n",
      "\u001b[1munwavering\u001b[0m | unwavering\n",
      "\u001b[1mprogram\u001b[0m | programs\n",
      "\u001b[1mday\u001b[0m | days\n",
      "\u001b[1mto\u001b[0m | to\n",
      "\u001b[1mlay\u001b[0m | \n",
      "\u001b[1mequity\u001b[0m | equity\n",
      "\u001b[1msystemic\u001b[0m | systemic\n",
      "\u001b[1mescape\u001b[0m | escape\n",
      "\u001b[1mour\u001b[0m | our\n",
      "\u001b[1msocial\u001b[0m | social\n",
      "\u001b[1mbe\u001b[0m | be\n",
      "\u001b[1mplace\u001b[0m | place\n",
      "\u001b[1mbuild\u001b[0m | build\n",
      "\u001b[1mfabric\u001b[0m | fabric\n",
      "\u001b[1mlet\u001b[0m | left let\n",
      "\u001b[1mmust\u001b[0m | must\n",
      "\u001b[1munleash\u001b[0m | unleashed\n",
      "\u001b[1minclusion\u001b[0m | inclusion\n",
      "\u001b[1mpossible\u001b[0m | possible\n",
      "\u001b[1mproblem\u001b[0m | problems\n",
      "\u001b[1mdefine\u001b[0m | define\n",
      "\u001b[1mrequire\u001b[0m | requires\n",
      "\u001b[1mof\u001b[0m | of\n",
      "\u001b[1maaa\u001b[0m | aaas\n",
      "\u001b[1mserve\u001b[0m | \n",
      "\u001b[1mharrowing\u001b[0m | harrowing\n",
      "\u001b[1msadness\u001b[0m | sadness\n",
      "\u001b[1mthan\u001b[0m | than\n",
      "\u001b[1mwherever\u001b[0m | wherever\n",
      "\u001b[1mtrust\u001b[0m | trust\n",
      "\u001b[1mscientist\u001b[0m | scientists\n",
      "\u001b[1mmake\u001b[0m | makes\n"
     ]
    }
   ],
   "source": [
    "# This will be the main() function\n",
    "\n",
    "\n",
    "dic = dict()\n",
    "root_list = []\n",
    "leaf_list = []\n",
    "\n",
    "# This code finds the root words and returns it as a list\n",
    "my_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\decode-diversity-statements\\texts\\AAAS_(American_Association_for_the_Advancement_of_Science).txt\", encoding=\"utf-8\")\n",
    "for line in my_file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    rem_ove = removePunctuation(line)\n",
    "    noStop = removeStopWords(rem_ove)\n",
    "    lemmaWord = lemmatizeWords(noStop)\n",
    "    for words in lemmaWord: #Task (done): add the lemmatized words with a loop so it doesn't create a list of list\n",
    "        root_list.append(words)\n",
    "\n",
    "root_list = list(set(root_list))  # removes the duplicate words\n",
    "\n",
    "\n",
    "\n",
    "#create a dictionory here assign all the roots to keys and empty list as values\n",
    "for root in root_list:\n",
    "    dic[root] = []\n",
    "\n",
    "    \n",
    "# This code finds the leaf words and returns it as a list  \n",
    "token_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\decode-diversity-statements\\texts\\AAAS_(American_Association_for_the_Advancement_of_Science).txt\", encoding=\"utf-8\")\n",
    "for line in token_file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    rem_ove = removePunctuation(line)\n",
    "    noStop = removeStopWords(rem_ove)\n",
    "    for words in noStop:\n",
    "        leaf_list.append(words)\n",
    "        \n",
    "leaf_list = list(set(leaf_list))  # removes the duplicate words\n",
    "\n",
    " \n",
    "# This code finds the leaf words and returns it as a list   \n",
    "for root in root_list:\n",
    "    #print(root)\n",
    "    for leaf in leaf_list:\n",
    "        #print(leaf)\n",
    "        similiarity = difflib.SequenceMatcher(None, root, leaf).ratio()\n",
    "        if similiarity > 0.80:\n",
    "            dic[root].append(leaf)\n",
    "\n",
    "\n",
    "#Task: visualization: print out the roots and leaves\n",
    "key_list = []\n",
    "value_list = []\n",
    "\n",
    "for k in dic.keys():\n",
    "    key_list.append(k)\n",
    "for v in dic.values():\n",
    "    value_list.append(v)\n",
    "print(\"\\033[1m\" + \"Root and Leaf Plot\".center(60) + \"\\n\")\n",
    "for key,value in zip(key_list,value_list):\n",
    "    print (\"\\033[1m\" + key + \"\\033[0m\" + ' | ' + \" \".join(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('striped', 'NN'), ('further', 'RB'), ('are', 'VBP'), ('impacted', 'VBN'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n",
      "['The', 'strip', 'far', 'be', 'impact', 'on', 'their', 'foot', 'for', 'best']\n",
      "['The', 'striped', 'further', 'be', 'impact', 'on', 'their', 'foot', 'for', 'best']\n",
      "happi\n"
     ]
    }
   ],
   "source": [
    "#Wordnet lemmatization functions\n",
    "def lemmatizeWords(sentence):\n",
    "    '''takes as an input tokenized sentence without punctuations. Returns lemmatized words'''\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    sentence = nltk.pos_tag(sentence)   #find POS in the sentence\n",
    "    for word, tag in sentence:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB'):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('JJ'):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='a'))\n",
    "        else:\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n",
    "\n",
    "sentence = \"The striped further are impacted on their feet for best\"\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "#print(word_list)\n",
    "print(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "print(lemmatizeWords(word_list))\n",
    "\n",
    "def lemmatizer(word, pos):\n",
    "    lemma = wordnet._morphy(word, pos)\n",
    "    if lemma:\n",
    "        return min(lemma, key=len)\n",
    "    else:\n",
    "        return word\n",
    "print(lemmatizer(\"happi\",wordnet.NOUN ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b71dad7c01d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"The striped bats were hanging on their feet and ate best fishes\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlemmatized_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#> ['striped', 'bat', 'be', 'hang', 'foot', 'eat', 'best', 'fish']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.utils import lemmatize\n",
    "sentence = \"The striped bats were hanging on their feet and ate best fishes\"\n",
    "lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]\n",
    "#> ['striped', 'bat', 'be', 'hang', 'foot', 'eat', 'best', 'fish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n",
      "happi\n",
      "happi\n",
      "['h', 'a', 'p', 'p', 'i']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "sn = SnowballStemmer(\"english\")\n",
    "\n",
    "stem = sn.stem(\"happiness\")\n",
    "print(lemma(stem))\n",
    "print(lemmatizer(stem,wordnet.VERB ))\n",
    "print(sn.stem(\"happy\"))\n",
    "print(lemmatizeWords(stem))\n",
    "# What I want to do:\n",
    "\n",
    "#print(some_unstem_function(\"happi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-11a68b036738>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-11a68b036738>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Try word_stemmer package, clone it from here and do pip install -e word_forms.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pip install -e word_forms.\n",
    "\n",
    "from word_forms.word_forms import get_word_forms\n",
    "get_word_forms('conclusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PorterStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-db176a50a9ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlancaster\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mporter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mstem_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mroot_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PorterStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "stem_list = []\n",
    "root_words = []\n",
    "test = [\"happiness\"]\n",
    "# word_list = [\"happy\", \"race\", \"collect\", \"arrive\", \"distribute\"]\n",
    "# compound_words = [\"happiness\", \"racism\", \"collective\", \"arrival\", \"distribution\", \"collection\"]\n",
    "# for word in compound_words:\n",
    "#     stem_list.append(lancaster.stem(word))\n",
    "# print(stem_list)\n",
    "    \n",
    "for c in stem_list:\n",
    "    #print(root)\n",
    "    for w in word_list:\n",
    "        #print(leaf)\n",
    "        similiarity = difflib.SequenceMatcher(None, c, w).ratio()\n",
    "        if similiarity > 0.80:\n",
    "            root_words.append(w)\n",
    "print(root_words)\n",
    "\n",
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "\n",
    "for word in test:\n",
    "    stem_list.append(lancaster.stem(word))\n",
    "print(stem_list)\n",
    "\n",
    "\n",
    "for t in test:\n",
    "    #print(root)\n",
    "    for w in word_list:\n",
    "        #print(leaf)\n",
    "        similiarity = difflib.SequenceMatcher(None, t, w).ratio()\n",
    "        if similiarity > 0.80:\n",
    "            if w[-3:] != \"ness\":\n",
    "                root_words.append(w)\n",
    "print(root_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'commun', 'collect', 'distribut', 'collect', 'success', 'impact', 'going']\n",
      "['happy', 'commune', 'collect', 'distribute', 'collect', 'success', 'impact', 'going']\n"
     ]
    }
   ],
   "source": [
    "file = open(r'C:\\Users\\manalais\\Box\\Web Developer\\Book1.csv')\n",
    "line = file.readlines()\n",
    "#print(line)\n",
    "new_file = open(r'C:\\Users\\manalais\\Box\\Web Developer\\root_words.txt',mode=\"a+\",encoding=\"utf-8\") \n",
    "\n",
    "# string = \"HEL\"\n",
    "# print(string.lower())\n",
    "# start = word.index('\"')\n",
    "root_list = []\n",
    "words = []\n",
    "# end = word.index(' ')\n",
    "for word in line:\n",
    "    if word.find('\"') or word.find('->'):\n",
    "        word = word.replace('\"', '') \n",
    "        word = word.replace('->', '')\n",
    "        word = word.replace('ï»¿', '')  \n",
    "        root_list.append(word)\n",
    "for i in root_list:\n",
    "    i = i.lower()\n",
    "    words.append((i[0:i.index(' ')]))\n",
    "    \n",
    "    \n",
    "# words = '\\n '.join(words)\n",
    "# new_file.writelines(words)\n",
    "# new_file.close()\n",
    "\n",
    "\n",
    "#lemmatize it\n",
    "# run it through this\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "#porter = PorterStemmer()\n",
    "stem_list = []\n",
    "root_words = []\n",
    "compound_words = [\"happiness\", \"Community\", \"collective\", \"distribution\", \"collection\", \"successfully\", 'impacted']\n",
    "for word in compound_words:\n",
    "    stem_list.append(lancaster.stem(word))\n",
    "print(stem_list)\n",
    "\n",
    "for c in stem_list:\n",
    "    for w in words:\n",
    "        similiarity = difflib.SequenceMatcher(None, c, w).ratio()\n",
    "        if similiarity > 0.91:\n",
    "            root_words.append(w)\n",
    "\n",
    "print(root_words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
