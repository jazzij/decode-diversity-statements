{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manalais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "import difflib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "def removePunctuation(sentence):\n",
    "    '''Input must be string. Tokenizes and removes punctuation.'''\n",
    "    \n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    punctuations=\"\\\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~–\"\n",
    "    for word in tokenized_sentence:\n",
    "        if word in punctuations:\n",
    "            tokenized_sentence.remove(word)\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove StopWords\n",
    "def removeStopWords(sentence):\n",
    "    '''takes tokenized words as an input and removes all the stop words'''\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in stopwords.words('english'):\n",
    "            sentence.remove(word)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizes words\n",
    "#Task: lemmatizeWords function needs to be improved to get root of ALL words\n",
    "def lemmatizeWords(sentence):\n",
    "    '''takes as an input tokenized sentence without punctuations. Returns lemmatized words'''\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    sentence = nltk.pos_tag(sentence)   #find POS in the sentence\n",
    "    for word, tag in sentence:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB'):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('JJ'):\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word, pos='a'))\n",
    "        else:\n",
    "            lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                     Root and Leaf Plot                     \n",
      "\n",
      "\u001b[1mfear\u001b[0m | fear\n",
      "\u001b[1mmust\u001b[0m | must must\n",
      "\u001b[1maxis\u001b[0m | axis\n",
      "\u001b[1mtime\u001b[0m | times time time\n",
      "\u001b[1mlack\u001b[0m | lack\n",
      "\u001b[1malong\u001b[0m | along\n",
      "\u001b[1mmission\u001b[0m | mission\n",
      "\u001b[1mhomophobia\u001b[0m | homophobia\n",
      "\u001b[1mracism\u001b[0m | racism\n",
      "\u001b[1munleash\u001b[0m | unleashed\n",
      "\u001b[1munderstandable\u001b[0m | understandable\n",
      "\u001b[1munbalanced\u001b[0m | unbalanced\n",
      "\u001b[1mleadership\u001b[0m | leadership leadership leadership\n",
      "\u001b[1mcolleague\u001b[0m | colleagues\n",
      "\u001b[1mbe\u001b[0m | be be\n",
      "\u001b[1mrelevant\u001b[0m | relevant\n",
      "\u001b[1msupport\u001b[0m | support support support\n",
      "\u001b[1munwavering\u001b[0m | unwavering\n",
      "\u001b[1mgo\u001b[0m | go\n",
      "\u001b[1mtogether\u001b[0m | together together together\n",
      "\u001b[1mone\u001b[0m | one one\n",
      "\u001b[1mscience\u001b[0m | science science science science science\n",
      "\u001b[1mnot\u001b[0m | not\n",
      "\u001b[1mprinciple\u001b[0m | principles principles principles\n",
      "\u001b[1mbare\u001b[0m | are bare\n",
      "\u001b[1mlast\u001b[0m | last\n",
      "\u001b[1mto\u001b[0m | to to\n",
      "\u001b[1mprovide\u001b[0m | provide\n",
      "\u001b[1mmore\u001b[0m | more\n",
      "\u001b[1mgood\u001b[0m | good\n",
      "\u001b[1min\u001b[0m | in in in\n",
      "\u001b[1mall\u001b[0m | all\n",
      "\u001b[1maaas\u001b[0m | aaas aaas\n",
      "\u001b[1mpull\u001b[0m | pull\n",
      "\u001b[1mcommunity\u001b[0m | community community community community community community\n",
      "\u001b[1minclusion\u001b[0m | inclusion inclusion\n",
      "\u001b[1mcommunities\u001b[0m | communities communities\n",
      "\u001b[1mwherever\u001b[0m | wherever\n",
      "\u001b[1mlet\u001b[0m | left let let\n",
      "\u001b[1mbegin\u001b[0m | begins\n",
      "\u001b[1mmake\u001b[0m | makes\n",
      "\u001b[1munchecked\u001b[0m | unchecked\n",
      "\u001b[1mconfident\u001b[0m | confident\n",
      "\u001b[1mweek\u001b[0m | weeks\n",
      "\u001b[1mdeeply\u001b[0m | deeply\n",
      "\u001b[1mprogram\u001b[0m | programs\n",
      "\u001b[1mno\u001b[0m | no\n",
      "\u001b[1mmany\u001b[0m | many\n",
      "\u001b[1msociety\u001b[0m | society society society\n",
      "\u001b[1ma\u001b[0m | a a a a\n",
      "\u001b[1menter\u001b[0m | enter\n",
      "\u001b[1mrededicate\u001b[0m | rededicate\n",
      "\u001b[1mescape\u001b[0m | escape\n",
      "\u001b[1mu\u001b[0m | \n",
      "\u001b[1msexism\u001b[0m | sexism\n",
      "\u001b[1msuccess\u001b[0m | successes\n",
      "\u001b[1mgeneration\u001b[0m | generation\n",
      "\u001b[1mcan\u001b[0m | can can can\n",
      "\u001b[1mrecommit\u001b[0m | recommit recommit\n",
      "\u001b[1mrequire\u001b[0m | requires\n",
      "\u001b[1meveryone\u001b[0m | everyone\n",
      "\u001b[1maround\u001b[0m | around\n",
      "\u001b[1mteam\u001b[0m | team\n",
      "\u001b[1mwe\u001b[0m | we we we we\n",
      "\u001b[1mthan\u001b[0m | than\n",
      "\u001b[1menterprise\u001b[0m | enterprise\n",
      "\u001b[1mdiversity\u001b[0m | diversity diversity\n",
      "\u001b[1mpain\u001b[0m | pain\n",
      "\u001b[1mchallenge\u001b[0m | challenges challenges\n",
      "\u001b[1mmonth\u001b[0m | months\n",
      "\u001b[1manswer\u001b[0m | answers\n",
      "\u001b[1msystemic\u001b[0m | systemic systemic\n",
      "\u001b[1mfeel\u001b[0m | feel\n",
      "\u001b[1mprinciples\u001b[0m | principles principles principles\n",
      "\u001b[1minjustice\u001b[0m | injustice\n",
      "\u001b[1msuccessfully\u001b[0m | successfully\n",
      "\u001b[1mproblem\u001b[0m | problems\n",
      "\u001b[1maaa\u001b[0m | aaas aaas\n",
      "\u001b[1mfabric\u001b[0m | fabric\n",
      "\u001b[1mensure\u001b[0m | \n",
      "\u001b[1mintervention\u001b[0m | intervention\n",
      "\u001b[1mplace\u001b[0m | place\n",
      "\u001b[1mimpacted\u001b[0m | impacted\n",
      "\u001b[1mstart\u001b[0m | starts\n",
      "\u001b[1mleft\u001b[0m | left let let\n",
      "\u001b[1mface\u001b[0m | face\n",
      "\u001b[1mnational\u001b[0m | national\n",
      "\u001b[1mtear\u001b[0m | tear\n",
      "\u001b[1madvocacy\u001b[0m | advocacy\n",
      "\u001b[1mcome\u001b[0m | come\n",
      "\u001b[1mseem\u001b[0m | seems\n",
      "\u001b[1mbring\u001b[0m | bring\n",
      "\u001b[1mharrowing\u001b[0m | harrowing\n",
      "\u001b[1mhope\u001b[0m | hope\n",
      "\u001b[1msafe\u001b[0m | safe\n",
      "\u001b[1mevery\u001b[0m | every ever\n",
      "\u001b[1mfoundation\u001b[0m | foundations\n",
      "\u001b[1madvance\u001b[0m | \n",
      "\u001b[1mserve\u001b[0m | \n",
      "\u001b[1mtorrent\u001b[0m | torrent\n",
      "\u001b[1mscientific\u001b[0m | scientific\n",
      "\u001b[1mthis\u001b[0m | this\n",
      "\u001b[1mpast\u001b[0m | past\n",
      "\u001b[1mdestroy\u001b[0m | destroy\n",
      "\u001b[1mof\u001b[0m | of of\n",
      "\u001b[1mcritical\u001b[0m | critical critical critical\n",
      "\u001b[1morganization\u001b[0m | organization organization\n",
      "\u001b[1memotion\u001b[0m | emotion\n",
      "\u001b[1mpublication\u001b[0m | publications\n",
      "\u001b[1mability\u001b[0m | ability\n",
      "\u001b[1mforce\u001b[0m | force force\n",
      "\u001b[1mequity\u001b[0m | equity equity\n",
      "\u001b[1mfuture\u001b[0m | future\n",
      "\u001b[1mour\u001b[0m | our our our our our our our our\n",
      "\u001b[1mdefine\u001b[0m | define\n",
      "\u001b[1mthese\u001b[0m | these these these\n",
      "\u001b[1mevent\u001b[0m | events\n",
      "\u001b[1mday\u001b[0m | days days\n",
      "\u001b[1muncertainty\u001b[0m | uncertainty\n",
      "\u001b[1mtrust\u001b[0m | trust\n",
      "\u001b[1mhave\u001b[0m | \n",
      "\u001b[1m’\u001b[0m | ’\n",
      "\u001b[1mhatred\u001b[0m | hatred\n",
      "\u001b[1mtroubled\u001b[0m | troubled\n",
      "\u001b[1msocial\u001b[0m | social\n",
      "\u001b[1mthe\u001b[0m | the the the the\n",
      "\u001b[1mneed\u001b[0m | needs\n",
      "\u001b[1mbuild\u001b[0m | build\n",
      "\u001b[1msteady\u001b[0m | steady\n",
      "\u001b[1mnevertheless\u001b[0m | nevertheless\n",
      "\u001b[1manger\u001b[0m | anger\n",
      "\u001b[1mhappen\u001b[0m | \n",
      "\u001b[1mscientist\u001b[0m | scientists\n",
      "\u001b[1mever\u001b[0m | every ever\n",
      "\u001b[1mdiscrimination\u001b[0m | discrimination\n",
      "\u001b[1mworld\u001b[0m | world world\n",
      "\u001b[1mchange\u001b[0m | change change\n",
      "\u001b[1mhelplessness\u001b[0m | helplessness\n",
      "\u001b[1mupon\u001b[0m | upon\n",
      "\u001b[1mnavigate\u001b[0m | navigate\n",
      "\u001b[1mfriend\u001b[0m | friends\n",
      "\u001b[1mbeyond\u001b[0m | beyond\n",
      "\u001b[1mrespect\u001b[0m | respect respected\n",
      "\u001b[1manother\u001b[0m | another another\n",
      "\u001b[1msadness\u001b[0m | sadness\n",
      "\u001b[1munprecedented\u001b[0m | unprecedented\n",
      "\u001b[1mpossible\u001b[0m | possible\n",
      "\u001b[1mcarry\u001b[0m | carry\n",
      "\u001b[1mlay\u001b[0m | \n",
      "\u001b[1madvocate\u001b[0m | advocates\n",
      "\u001b[1mprogress\u001b[0m | progress\n"
     ]
    }
   ],
   "source": [
    "# This will be the main() function\n",
    "\n",
    "\n",
    "dic = dict()\n",
    "root_list = []\n",
    "leaf_list = []\n",
    "\n",
    "# This code finds the root words and returns it as a list\n",
    "my_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\decode-diversity-statements\\texts\\AAAS_(American_Association_for_the_Advancement_of_Science).txt\", encoding=\"utf-8\")\n",
    "for line in my_file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    rem_ove = removePunctuation(line)\n",
    "    noStop = removeStopWords(rem_ove)\n",
    "    lemmaWord = lemmatizeWords(noStop)\n",
    "    for words in lemmaWord: #Task (done): add the lemmatized words with a loop so it doesn't create a list of list\n",
    "        root_list.append(words)\n",
    "\n",
    "root_list = list(set(root_list))  # removes the duplicate words\n",
    "\n",
    "\n",
    "\n",
    "#create a dictionory here assign all the roots to keys and empty list as values\n",
    "for root in root_list:\n",
    "    dic[root] = []\n",
    "\n",
    "    \n",
    "# This code finds the leaf words and returns it as a list  \n",
    "token_file = open(r\"C:\\Users\\manalais\\Box\\Web Developer\\Decoding Racism\\decode-diversity-statements\\texts\\AAAS_(American_Association_for_the_Advancement_of_Science).txt\", encoding=\"utf-8\")\n",
    "for line in token_file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    rem_ove = removePunctuation(line)\n",
    "    noStop = removeStopWords(rem_ove)\n",
    "    for words in noStop:\n",
    "        leaf_list.append(words)\n",
    "\n",
    " \n",
    "# This code finds the leaf words and returns it as a list   \n",
    "for root in root_list:\n",
    "    #print(root)\n",
    "    for leaf in leaf_list:\n",
    "        #print(leaf)\n",
    "        similiarity = difflib.SequenceMatcher(None, root, leaf).ratio()\n",
    "        if similiarity > 0.80:\n",
    "            dic[root].append(leaf)\n",
    "\n",
    "\n",
    "#Task: visualization: print out the roots and leaves\n",
    "key_list = []\n",
    "value_list = []\n",
    "\n",
    "for k in dic.keys():\n",
    "    key_list.append(k)\n",
    "for v in dic.values():\n",
    "    value_list.append(v)\n",
    "print(\"\\033[1m\" + \"Root and Leaf Plot\".center(60) + \"\\n\")\n",
    "for key,value in zip(key_list,value_list):\n",
    "    print (\"\\033[1m\" + key + \"\\033[0m\" + ' | ' + \" \".join(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
